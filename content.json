{"posts":[{"title":"Useful Documents and Blogs","text":"I will maintain a list of blogs or official document interesting to me here. 1. LLVM Kaleidoscope: Implementing a Language with LLVM Writting a LLVM Pass How to Parse LLVM IR Line by Line? 2. Program Analysis Split Bitcode Split input: Input Splitting for Cloud-Based Static Application Security Testing Platforms (ICSE’ 22) Related to C2Rust C2Rust: Ownership Analysis [link] SPrinter: A Static Checker for Finding Smart Pointer Errors in C++ Programs [link] Detecting Memory-Related Bugs by Tracking Heap Memory Management of C++ Smart Pointers [link] Translating C to Safer Rust (OOPSLA’21) [link] Rust Lifetime Rust Lifetime [link] Visualization of Lifetime Constraints in Rust [link] Clearblue 3. Architecture Gem5 Simulator: Official Document 4. Blockchain Gas Cost Bound Analysis Developing Cost-Effective Blockchain-Powered Applications: A Case Study of the Gas Usage of Smart Contract Transactions in the Ethereum Blockchain Platform 5. Fair Division Group Fairness Kneser graph: Discrepency theory: Almost Envy-Freeness for Groups: Improved Bounds via Discrepancy Theory Minimize the number of queries when calculating fair allocations Fairly Allocating Many Goods with Few Queries The Query Complexity of Cake Cutting Almost envy-free allocations with connected bundles","link":"/2023/02/13/Useful-Documents-and-Blogs/"},{"title":"SSA Transformation of Move Stackless Bytecodes","text":"最近在做有关Move Prover的开发。这两周主要负责将Move Stackless Bytecode转换为single staic assignment(SSA)格式。想简单写一篇文章记录一下具体实现流程。 1. Source Codes SSA源代码可以在fork出的repo里最新的两个commit获取。 Usage 1$ move prove -p [move dir] -- -ssa #enable ssa transformation Example: 1234IfCondition├── Move.toml└── sources └── main.move 其中main.move的内容如下： 1234567891011module 0xCAFE::BasicCoin { public fun f1(x1: u8) :u8 {x1} public fun if1(x1: u8, i: u8) :u8 { if (i &gt; 0) { x1 = 1; }; f1(x1) }} 执行 1$ move prove -p IfCondition -- -dump-bytecode 可以看到原始的bytecode如下: 123456789101112131415161718192021public fun BasicCoin::if1($t0|x1: u8, $t1|i: u8): u8 { var $t2: u8 var $t3: u8 var $t4: bool var $t5: u8 var $t6: u8 var $t7: u8 0: $t2 := move($t1) 1: $t3 := 0 2: $t4 := &gt;($t2, $t3) 3: if ($t4) goto 4 else goto 8 4: label L1 5: $t5 := 1 6: $t0 := $t5 7: goto 8 8: label L0 9: $t6 := move($t0) 10: $t7 := BasicCoin::f1($t6) 11: return $t7} 执行 12$ move prove -p IfConidtion -- -ssa 输出的经ssa转换后的bytecode如下： 12345678910111213141516171819202122public fun BasicCoin::if1($t0|x1: u8, $t1|i: u8): u8 { var $t2: u8 var $t3: u8 var $t4: bool var $t5: u8 var $t6: u8 var $t7: u8 var $t8: u8 var $t9: u8 0: $t2 := move($t1) 1: $t3 := 0 2: $t4 := &gt;($t2, $t3) 3: if ($t4) goto 4 else goto 7 4: label L1 5: $t5 := 1 6: $t9 := $t5 7: label L0 8: $t8 := phi($t0, $t9) 9: $t6 := move($t8) 10: $t7 := BasicCoin::f1($t6) 11: return $t7} 2. Main Algorithm 主要的算法实现参考了 [1] 中基于dominator的SSA construction算法。大致流程分为两步： (Phi Insertion): 先计算出控制流图中需要插入Phi函数的位置。 (Variable Renaming): 对源代码中定义和使用的变量做重命名。 3. Implementation 目前move-prover在stackless bytecode这个阶段进行的分析，都是以FunctionProcessor的形式给上层调用提供接口。其中FunctionProcessor::Process()方法，在拿到所有函数的signature，环境信息，以及某个具体待分析函数的具体内容后，对待分析的函数内容做一些变换/分析。 move-prover/src/bytecode/src/function_target_pipeline.rs12345678910111213pub trait FunctionTargetProcessor { /// Processes a function variant. Takes as parameter a target holder which can be mutated, the /// env of the function being processed, and the target data. During the time the processor is /// called, the target data is removed from the holder, and added back once transformation /// has finished. This allows the processor to take ownership on the target data. fn process( &amp;self, _targets: &amp;mut FunctionTargetsHolder, _fun_env: &amp;FunctionEnv&lt;'_&gt;, _data: FunctionData, ) -&gt; FunctionData { unimplemented!() } 出于与move-prover后端代码开发保持一致，并且能让导出的SSA codes可以用上目前已有的分析的目的，我将SSA转化也实现在了FunctionTargetProcessor内。但其实注意到，process的返回值只是原本的data，这是因为，经过SSA转换后的bytecode和原本的bytecode其实是两套，目前还不确定会不会影响一些built-in的analysis的正常执行。所以在这里只是把转换后的结果打印出来了，并没有实际返回。 move-prover/src/bytecode/src/ssa_analysis.rs1234567891011121314151617impl FunctionTargetProcessor for SSAConstructionProcessor { fn process( &amp;self, _targets: &amp;mut FunctionTargetsHolder, func_env: &amp;FunctionEnv, data: FunctionData, ) -&gt; FunctionData { let original_data = data.clone(); let mut define_once_vars: Vec&lt;TempIndex&gt; = vec![]; let data_with_phi = Self::insert_phi_functions(data, func_env, &amp;mut define_once_vars); let data_after_renaming = Self::rename_variables(data_with_phi, func_env, &amp;mut define_once_vars); let func_target = FunctionTarget::new(func_env, &amp;data_after_renaming); println!(&quot;{}&quot;, format!(&quot;{func_target}&quot;)); original_data } 3.1 Add Phi() to the Definition of Bytecodes SSA和原始bytecode的主要区别在于Phi函数的引入。我在原始bytecode的定义中，加入了一条$v_s = \\phi\\left( [v_{t,1}, \\ldots, v_{t, k}], v\\right)$。其中$v_s$表示由$\\phi$函数定义的新寄存器编号，$[v_{t,1}, \\ldots, v_{t, k}]$表示在此处合并的所有变量定义，$v$表示这些新变量在原始bytecode中对应的下标。 move-prover/src/bytecode/src/sackless_bytecode.rs123456pub enum Bytecode { Assign(AttrId, TempIndex, TempIndex, AssignKind), ... Phi(AttrId, TempIndex, Vec&lt;TempIndex&gt;, TempIndex), // New added} Note: 实际上，尽管bytecode中目前添加了phi这条语句，在其他分析中仍然用的是旧的bytecode。即使在语法上不得不处理phi，也都采取跳过的办法。 3.2 Graph Algorithm API 目前在mover-prover里，提供了两种版本的图算法，一种是move-prover/src/bytecode/src/graph.rs内提供的一些general的图算法的实现（dominator计算，immediate dominance tree的构建），另外一种是在move-prover/src/bytecode/src/stackless_control_flow_graph.rs内提供了一些control flow graph上的算法（如何从线性的bytecode转成control flow graph，如何打印等等）。 在拿到一个StacklessControlFlowGraph后，如果想在上面跑一些图算法，需要先将其转为一个graph，具体代码如下： move-prover/src/bytecode/src/ssa_analysis.rs123456789101112131415fn create_graph_from_cfg(cfg: &amp;StacklessControlFlowGraph) -&gt; Graph&lt;BlockId&gt; { let entry: BlockId = cfg.entry_block(); let nodes: Vec&lt;BlockId&gt; = cfg.blocks(); let edges: Vec&lt;(BlockId, BlockId)&gt; = nodes .iter() .flat_map(|x| { cfg.successors(*x) .iter() .map(|y| (*x, *y)) .collect::&lt;Vec&lt;(BlockId, BlockId)&gt;&gt;() }) .collect(); let graph = Graph::new(entry, nodes, edges); graph} 和dominator相关的计算都实现在move-prover/src/bytecode/src/graph.rs的DomRelation内。它在拿到一个图之后，对结点进行重新编号。具体来说，假如输入的图的所有结点为$[a, b, c]$，DomRelation会内部维护一个映射，将$[a, b, c]$映射到$[0, 1, 2]$。其中主要用到的API接口的介绍如下： pub fn is_dominated_by(&amp;self, x: T, y: T) -&gt; bool 判断结点x是否被结点y所dominate。 pub fn postorder_visit(&amp;mut self, graph: &amp;Graph&lt;T&gt;) 对输入的图做后续遍历，并初始化内部维护的映射。 pub fn depth_first_traverse(&amp;self, graph: &amp;Graph&lt;T&gt;) -&gt; Vec&lt;T&gt; 对immediate dominante tree做深度优先搜索，并返回遍历结果。 pub fn compute_dominance_frontier(&amp;mut self, graph: &amp;Graph&lt;T&gt;) 计算所有结点对应的dominance frontier。 3.3 Phi Insertion Phi insertion的主要实现大致如下： 先取出原始的data: FunctionData内的所有bytecode，并生成对应的control flow graph， 1234let old_code = std::mem::take(&amp;mut builder.data.code);let cfg = StacklessControlFlowGraph::new_forward(&amp;old_code);let graph = Self::create_graph_from_cfg(&amp;cfg);let dom_frontier = graph.compute_dominance_frontier(); 执行[1]中的Phi insertion算法，计算出所有需要插入Phi的位置在bytecode中的offset 逐句遍历bytecode，并一条一条emit到data内， 12345678910111213for (offset, bc) in old_code.into_iter().enumerate() { let co_offset = offset as CodeOffset; if phi_insertion_records.contains_key(&amp;co_offset) { builder.emit(bc); let variable_vec = phi_insertion_records.get_mut(&amp;co_offset).unwrap(); for v in variable_vec { let new_attrid = builder.new_attr(); builder.emit(Bytecode::Phi(new_attrid, v.clone(), vec![], v.clone())) } } else { builder.emit(bc); }} 3.4 Variable Renaming Variable renaming的主要实现大致如下： 先取出原始的data: FunctionData内的所有bytecode，并生成对应的control flow graph。 初始化每个寄存器的reaching definition， 123456for i in 0..parameter_count { reaching_def_map.insert(i, (i, cfg.entry_block()));}for i in parameter_count..local_count { reaching_def_map.insert(i, (usize::MAX, cfg.entry_block()));} DFS遍历immediate dominance tree，并对每个block内的指令做renaming, 123456789101112let dom_tree_dfs_res = dom_relation.depth_first_traverse(&amp;graph);for block_id in dom_tree_dfs_res { Self::rename_variables_within_block( &amp;mut builder, &amp;mut old_code, &amp;dom_relation, &amp;mut reaching_def_map, &amp;cfg, &amp;block_id, define_once_vars, );} 4. Limitation 目前虽然实现里，经过SSA转化之后，得到的形式是FunctionData的格式。但目前还没有经过测试，看是否和其他分析兼容。 在SSA转化过程中，并没有处理Prop Reference [1]. Sec 3. SSA book.","link":"/2023/04/17/SSA-Transformation-of-Move-Stackless-Bytecodes/"},{"title":"Paper Reading: Input Splitting for Cloud-Based Static Application Security Testing Platforms","text":"简介： 这篇博客主要介绍FSE’22的一篇工作《Input Splitting for Cloud-Based Static Application Security Testing Platforms》。主要想法是通过切分输入，以减轻程序分析的时间复杂度。文章通过实验表明，按照他们的策略切分输入代码确实可以显著地提高执行效率，并且不会明显影响工具的精度。 关键词： Split Code, Static Analysis, Software Security 可扩展性一直是静态分析的一个热点。如何快速处理大规模代码（百万行以上），同时又保有一定的精确度，一直是近些年静态分析领域的一个研究热点。很多现有的工具在处理大规模代码时，往往会因为路径爆炸等原因，导致分析超时，内存开销过大等问题。 直观来说，将代码切分为若干片段后，可以有效避免程序分析过程中出现的路径爆炸等现象。但与此同时，还是会带来很多问题： 一方面，程序分析的准确确度 会受到影响。如果将代码切分为很多独立的片段，彼此之间的依赖关系会丧失。例如，某些错误可能是由于不同片段代码内类之间的相互调用而导致的。另外，例如可能抛出异常函数和对应的handler被划分在不同片段内，会导致false positive。 另一方面，某些程序分析方法的时间复杂度并一定和代码体积相关。例如，在数据流分析方法中，假如data-flow facts最终全部汇聚到一处，切分输入代码基本对复杂度没什么影响。 另外， 1. 研究动机 作者举了一个实际的代码分析工具的benchmark 「OWASP」作为例子。OWASP是一个用java编写的一系列测试样例，用来给测试工具的速度和准确性进行评分。其中，一共有$2740$个测试样例，每个测试样例都位于一个单独的java文件中。除此之外还有$162$个文件，实现了一些共用的工具函数。 在表 1中，给出了一些测试工具在该benchmark下的得分和耗时。可以看到，某些重量级的程序分析工具（例如，Commercial SAST-02）耗时非常久。如果每次客户的代码都需要等候这么长时间才能完成检查，显然是不太现实的。如果能将所有的测试文件进行划分，将测试划分为一些固定大小的集合， [example1.java]1234567891011121314151617181920public class Thing1 implements ThingInterface { @Override public String doSomething(String i) { String r = i; return r; }}// Simplified version of the OWASP BenchmarkTest 01025public class BenchmarkTest01025 extends HttpServlet { @Override public void doPost(HttpServletRequest req, HttpServletResponse response) throws Exception { String p = req.getHeader(&quot;foo&quot;); String bar = new Thing1().doSomething(p); File fileTarget = new File(&quot;./tmp&quot;, bar); response.getWriter().println(&quot;...&quot;); }} 2. 划分策略 假设我们需要分析程序$P$，其包含了$n$个文件$F= (f_1, \\ldots, f_n)$。我们希望将其划分为$m$个集合$R = (r_1, \\ldots, r_m)$，使得他们中的每个最多包含$S$个文件，并且他们的并是$F$。 算法总览 算法的第一步，是为每个文件创建一个集合，包含了文件本身和它在文件依赖图里距离小于等于$k$的所有文件（其中$k$是用户输入的参数），例如对于example1.java，初始的划分为(BenchmarkTest01025, Thing1, ThingInterface）和（Thing1, ThingInterface)。但这样划分很有可能带来的一个问题就是，每个集合的大小未必不超过$S$。 因此，在第二步中，我们对初始得到的划分进行拆分，保证没有大小超过$S$的集合。这一步可能会丧失一定的精度。但我们希望在拆分过程中，能尽可能的保留一些重要的文件。 在第三步中，作者对划分做了一些优化。主要包含对冗余集合的划分（例如，若文件集合$A$完全包含在$B$中，只需要对$B$做检查即可），合并一些小的集合（如果文件集合$A$和文件集合$B$的并依然大小不超过$S$，可以对其进行合并）。当然合并的方式不唯一，最终优化目标是希望能够使集合总数最小。 例子 用论文中给出的一个例子（图2展示了初始的依赖图）来具体说明一下算法的具体执行。用六个点$A, \\ldots, F$表示六个文件。 令$k=2$，首先执行第一步，得到初始的划分$(A, B, C)$, $(B, C)$, $©$, $(D, B, C)$, $(E, A, B, C, D, F)$, $(F)$。 就如之前说的那样，接下来需要将一些大的集合进行拆分。但拆分的过程中，又不希望损失掉太多的精度。因此，就需要保留一些看起来“比较重要”的文件。怎么衡量重要呢？在论文中，作者用结点的度数来衡量节点的重要程度。换句话说，一个文件被很多文件引用，那说明它一定是很重要的。 假设给定集合$r$，使得$|r| &gt; S$。论文中的方法是选取度数最高的$p_r = \\lfloor p\\cdot |r| \\rfloor$个点（如果$p_r &gt; S$，就选取$p_r = \\lfloor p\\cdot S \\rfloor$个节点），将这些点其表示为hdn。将$r$中的剩余点表示为ldn。随后将ldn均分为一系列文件ldn$_0$, $\\ldots$, ldn$_k$，使得每个集合最多只有$S-p_r$个点。 随后，在原始的划分中去掉$r$，并加入hdn$\\cup$ ldn$_0$, hdn$\\cup$ ldn$_1$, $\\ldots$, hdn$\\cup$ ldn$_k$。","link":"/2023/02/11/Paper-Reading-Input-Splitting-for-Cloud-Based-Static-Application-Security-Testing-Platforms/Paper-Reading-Input-Splitting-for-Cloud-Based-Static-Application-Security-Testing-Platforms/"},{"title":"Infer Buffer Overrun (II) - Abstract Domain","text":"Abstract In this blog, we will take a further look at the abstract domain within the buffer overrun checker (inferbo), including intervals, bounds, locations, and memory model. Inferbo uses the conventional program analysis techinque of intervals for the range of index values and buffer sizes. Infers works on the intervals [low, high], where the two bounds are both symbolic [1]. Moreover, the bounds can also contain some linear arithmetic with min/max operators. 1. Bound Here is the formal definition of bound within inferbo. It is implemented in the file infer/src/bufferoverrun/bounds.ml. $$ \\mathbf{LinearSym} \\triangleq \\mathbf{Sym} \\rightarrow \\mathbb{Z} $$ $$ \\mathbf{Bound} \\triangleq \\pm \\infty \\ |\\ \\mathbb{Z} + \\mathbf{LinearSym} \\ |\\ \\mathbb{Z} +\\max/\\min(\\mathbb{Z}, \\mathbf{LinearSym}) $$ $$ |\\ \\min/\\max(\\mathbf{Bound}, \\mathbf{Bound})\\ |\\ \\mathbb{Z} + \\mathbf{Bound} \\times \\mathbf{Bound}, $$ where $\\mathbf{Sym}$ represents symbols (e.g., variables occurred in the program), and $\\mathbf{LinearSym}$ represents a linear composition of symbols. Within the module Bound, infer also offers some tool functions. Among them, pp specifies the output format; of_big_int converts an int number n to Linear n; is_symbolic determines whether there is a symbol within a bound; le gives an rough comparation between two bounds. However, it only defines orders for only straightforward cases. For some comparation like $1+ x \\overset{?}{&lt;} 1+ y$, it can not determine the result and just return false; neg takes a bound to be negative; exact_min computes the exact minimum value of two bounds. For example, $\\mathrm{exact\\ min}(c1, c2 + x) = \\min(c2, \\min(c2 - c1, x))$. underapprox_min gives a lower bound of the minimum value of two bounds; overapprox_min gives an upper bound of the minimum value of two bounds; plus_u (plus_l) sums two bounds as upper bounds (lower bounds); 2. Interval The interval is defined in the module ItvPure of the file infer/src/bufferoverrun/itv.ml. Each interval is composed of a lower bound lb and an upper bound ub. 3. Memory Model Similar to the heap model in separation logic, the memory within infer buffer overrun checker is modeled as $\\mathbf{Mem}: \\mathbf{Loc} \\rightarrow \\mathbf{MVal}$, where $\\mathbf{Loc}$ represents either a stack location or a heap location location and $\\mathbf{MVal}$ represents the symbolic value stored at that location. 3.1 Location In infer/src/bufferoverrun/absLoc.ml, Loc is defined as follows, 1234567891011121314151617181920module Allocsite = struct type t = | Unknown | Symbol of Symb.SymbolPath.partial | Known of { proc_name: string ; caller_pname: Procname.t option ; node_hash: int ; inst_num: int ; dimension: int ; represents_multiple_values: bool ; path: Symb.SymbolPath.partial option } | LiteralString of stringendmodule Loc = struct type prim = Var of Var.t | Allocsite of Allocsite.t [@@deriving compare, equal] type t = prim BoField.t [@@deriving compare, equal] ...end Allocite is defined as the location of an instruction that allocates memory. Loc has two constructors. The first one represents a variable or a register while the second one represents an allocated memory location. Specifically, $$ \\mathbf{prim} \\triangleq \\mathbf{Var}\\ |\\ \\mathbf{Allocsite}, \\mathbf{Loc} \\triangleq \\mathbf{prim}\\ |\\ \\mathbf{prim}.field\\ |\\ *\\mathbf{prim} $$ 3.2 Value","link":"/2023/02/04/Infer-Buffer-Overrun-II/"},{"title":"Infer Buffer Overrun (I) - Build and Run","text":"Abstract Hello, everyone! This is a series of blogs. I want to discuss the structure of the buffer overrun checker of infer, a static analysis tool developed by Facebook. Now, I will firstly discuss how to build it and run checkers within it. 1. How to build infer? The official document both offers methods to get binary version and directly build from source codes. Building from source codes is a time-consuming job. So if you do not need to modify the source codes, you could just install the binary relases. 1.1 Get the binary version On Macos, $ brew install infer On Linux, to install infer with VERSION=0.XX.Y, $ VERSION=0.XX.Y; \\ curl -sSL &quot;https://github.com/facebook/infer/releases/download/v$VERSION/infer-linux64-v$VERSION.tar.xz&quot; \\ | sudo tar -C /opt -xJ &amp;&amp; \\ sudo ln -s &quot;/opt/infer-linux64-v$VERSION/bin/infer&quot; /usr/local/bin/infer 1.2 Build from source codes I have tried to build infer on arm based macos. Unfortunately, I could not build clang on M1-chip mac. Hence, I only use infer to check java codes on macos. Nevertheless, if you still would like to use infer on macos, you could choose the docker image I made. On Ubuntu, please first install the packages required. The details can be found at here. Then, follow the below steps, $ git clone https://github.com/facebook/infer.git $ cd infer $ ./build-infer.sh clang # Compile cpp $ sudo make install # ...or, alternatively, install Infer into your PATH $ export PATH=`pwd`/infer/bin:$PATH Note that, infer will default build a java version. Hence, if you do not want to build the cpp version, just replace the third command with ./build-infer.sh. The common error I met is ‘fail to ld’ and ‘not enough space on the disk’. The reasons and solutions are as follows: The first one is because clang is compiled in parallel. Hence, it would produce many intermediate files, making the swap and memory full. To avoid parallel compilation or reduce the number of threads, you could change the value of JOBS to 1 in the script facebook-clang-plugins/clang/setup.sh. The second one is because you may repeatly build infer several times. Therefore, there will be several unbuilt clang within /tmp/. You could just delete the previous files or restart your machine. On Macos, to build java-based infer, first install the required packaegs. $ brew install autoconf automake cmake opam pkg-config sqlite gmp mpfr java Then, follow the same steps as Linux. 2. Usage of Infer As the official document said, there are two phases of infer: capture and analyze. Within the first phase, infer compiles the source files and generates some temporary files for further analysis. All the intermediate files are stored in the directory infer-out/. After that, infer takes the files within infer-out/ as input and then, analyzes them and report errors. 2.1 How to run infer? For instance, to analyze a cpp source file hello.cpp, $ infer capture -- clang hello.cpp $ infer analyze -- clang hello.cpp # ... or, alternatively, just run the following command instead of the above two $ infer run -- clang hello.cpp If you do not specify checkers within the above command, infer will only do some basic checks. To activate some particular checkers, please add specific option, like --bufferoverrun (checking whether there exists a buffer ovrrun bug), --cost (computing the time complexity of functions and methods), etc. 2.2 How to find the trace triggering bugs? Infer collects the trace that triggers a bug. To see the trace, just run infer explore after running the command within 2.1 and write the number of issue. $ infer explore 1 # 1 can be replaced by other issue number 2.3 Tips for debugging Infer offers a convenient analysis framework for programmers to develop multiple checkers. It uses the technique of abstract interpretations, in which there is an abstract domain, and the checker computes the abstract value before and after running each instruction. A direct way to debug infer is to print the abstract values to the console. We can observe that, within most modules, there is a function called pp. This function specifies the format for printing this data structure. To print it, please plug this function as an argument to Logging.debug_dev. For example, if instr is a variable with type Sil.instr, to print it, just insert the following code into the source codes 1Logging.debug_dev &quot;Instr: %a&quot; Sil.instr.pp instr","link":"/2023/02/04/Infer-Buffer-Overrun-I/"},{"title":"Hello World","text":"Welcome to my website! This is your very first post.","link":"/2023/02/03/hello-world/"}],"tags":[{"name":"infer","slug":"infer","link":"/tags/infer/"},{"name":"static analysis","slug":"static-analysis","link":"/tags/static-analysis/"},{"name":"Program Analysis","slug":"Program-Analysis","link":"/tags/Program-Analysis/"},{"name":"SSA","slug":"SSA","link":"/tags/SSA/"},{"name":"Paper Reading","slug":"Paper-Reading","link":"/tags/Paper-Reading/"},{"name":"Static Analysis","slug":"Static-Analysis","link":"/tags/Static-Analysis/"},{"name":"Software Security","slug":"Software-Security","link":"/tags/Software-Security/"}],"categories":[{"name":"static anlysis","slug":"static-anlysis","link":"/categories/static-anlysis/"},{"name":"Program Analysis","slug":"Program-Analysis","link":"/categories/Program-Analysis/"},{"name":"notes","slug":"notes","link":"/categories/notes/"}],"pages":[{"title":"Publication","text":"Publications On Existence of Truthful Fair Cake Cutting Mechanisms (AIJ’2023 CCF-A). [journal version] (⭐ New!) Joint with Xiaolin Bu, Biaoshuai Tao (alphabetical author ordering). Fair Division with Prioritized Agents (AAAI’2023 CCF-A). [arxiv version] Joint with Xiaolin Bu, Zihao Li, Shengxin Liu*, Biaoshuai Tao (alphabetical author ordering). Scalable Linear Invariant Generation with Farkas’ Lemma (OOPSLA’2022 CCF-A). [conference version] Joint with Hongming Liu, Hongfei Fu*, Zhiyong Yu, Guoqiang Li. Workshop On the Complexity of Maximizing Social Welfare within Fair Allocations of Indivisible Goods (WINE’2022 CCF-A). [arxiv version] Joint with Xiaolin Bu, Zihao Li, Shengxin Liu*, Biaoshuai Tao (alphabetical author ordering). Working Papers Towards Denotational-semantics-based Compiler Correctness Verification. Joint with Zhang Cheng, Qinxiang Cao*.","link":"/publications/index.html"},{"title":"About","text":"I am currently a fourth-year undergraduate student in IEEE pilot class, Shanghai Jiao Tong University. In the fall of this year, I will go to HKUST to pursue PhD degree. Teaching Assistant Algorithm Design and Analysis (SJTU), 2022 Spring. Linear and Convex Optimization (SJTU), 2022 Fall. Research Interests My research interests include program static analysis and theoretical computer science problems. More recenltly, I have been studying problems in: Compilation Optimization GPGPU Online Fair Division In 2022 summer, we organized a paper reading group for discussing recent algorithm papers. The web page is here. If you have similar research interests to me, please don’t be shy to contact me. Awards Zhiyuan Honor Scholarship, SJTU (2019 - 2022, ¥5k/year) Huatai Scholarship, SJTU (2022, ¥1w/year) Class B Scholarship, SJTU (2021, 2022, ¥1k/year) Class C Scholarship, SJTU (2020, ¥500/year) Chenhao Alumni Scholarship, SJTU (2021, ¥5k/year) Contact mail: sjtu_xiaosong@sjtu.edu.cn","link":"/about/index.html"}]}